# Protocol 2019-05-08, Wednesday

## Papers/Technicalities

* Breiman, L. (2001). Statistical modeling: The two cultures (with comments and a rejoinder by the author). *Statistical science, 16*(3), 199-231. https://projecteuclid.org/download/pdf_1/euclid.ss/1009213726


## Protocol

* List of shortcuts used in the protocol
  * ML = machine learning
  * AI = artificial intelligence
  * VS/vs = versus
  * SVM = support vector machine(s)

### SESSION START
### Discussion Paper: Breiman, L. (2001).
Major topic: What is the discrepancy between "two worlds" (stat modelling VS data-driven) and why does it happen?

* Starting question in the plenum: The paper was written in year 2001 -> is the problem still there?
  * Yes, in academia, the most of the courses are firmly on the modelling side of things, so they are still taught mostly.
  * Some/many academical statistitians completely ignored ML, which made ML still not integrated with academical stats.
  * But also depends on person/department. In the methods department - it comes slowly.
  * Also, for example, seminars on ML are being taught more and more in masters of statistics, methods, etc.
* Some fields simply had to switch to ML to get better results
  * E.g., in political science/forecasting -> ML methods -> better forecasts/polling.
* A sidenote -- how to explain the fundamental difference in these two approaches?
  * Prediction VS Understanding
    * In the ML, you have a "black box" of the processes behind the phenomena observed and you're just interested in prediction.
    * In "classical" stats, you want to know what's happening -- what are the parameters, causal relationships.
* Small Area Estimation (SAE)
  * Example: How to estimate how many people in Germany live in poverty. Let n = 1000.
    * Then: Every federal state, every district... n_k is not enough anymore.
    * SAE is the combination of both (TOB. NACHFRAGEN, WAS ER MEINTE)?
* Opinion: Maybe this distinction is legit? Namely, (academical) stats have their goals, and business -- their own.
  * Contra-argument: Stats should also develop methods that are then applied in the field. Not only for "profit", but in medicine, etc.
* Question to plenum: Can you think of cases where the ML approach actually shed some light on what's happening "inside"? Namely, some vars are more predictive than others -> there is causality maybe, etc.?
  * Person 1: Yes, for example, regression tree. But most of the stuff happening in ML approach are pretty "black box-y".
  * Also some developments in ML that actually explicitly try to understand what's happening in your neural net. But again, it's not necessary.
* In policy prediction + ML there's problem that you always have to have a training and exp. data set.
  * Sometimes impossible at all (you can't have the same economy every year, etc.). Sometimes unethical.
* A relevant argument that is not made by author: If you assume that in our world there ARE some models that could predict the world as it best.... =>
  * If these models make better predictions, they MUST capture more reality, [independent from the approach behind them]. Isn't it an argument for studying ML approaches more in the academic context?
  * Response: Yes, but don't forget about parsimony/economy. We need to reduce complexity, reduce dimensionality ... ? [protocol not clear]
  * Generalazibility assumption (cf. ?Karl Popper's? trivial induction).
* Another interesting point of the author: You should always care about predictive error rate [in the long run]. 
  * Oracle property: number of observations and/or timepoints -> inf, then choose the variable(s) that work better.
* The problem of multiplicity of good models, that we discussed in the last session, is relevant here as well:
  * E.g., some says how cool is the model based on the prediction, but doesn't care if 100 other models predicted the same or even better.
  * ???Also, imagine having 7 variables, effect yes/no, if you go from the fact that you know???
  * Then you have 2^(6+5+4+3+2+1) possible combinations of all relationships
* Question from plenum: The author writes that his preferred method is random forests. Does it still hold?
  * Some new important methods, esp. for complicated cases (convolution networks, etc.)
  * But for images, text, the classic ?CART? and random forests are great. Often a combination of boosting, ..., random forests, etc. And then you look what worked better.
  * Also you look how you create variables: do you dichotomise some, etc.
  * So yes, RT still used a lot, esp. in combination with other methods. On the contrary, there were some topics that were popular back in day, but now kinda died out.
  * E.g., support vector machines, which was a hot topic back in time, but then everyone cooled down, saw that they are not that effective, then deep learning came in and everyone was impressed. Although a lot of PhDs have already been written on the SVM.
* Point about increasing dimensionality leading to better predictions - a questionable point, since:
  * Not mentions the bias/variance trade-off at all.
  * Although for his examples in the paper, he is absolutely right - back in the day the methods he's suggesting would work much better.
* Discussion of comments on paper
  * Cox was very critical, stated that the thesis is overblown. Prediction is of course an important part, but not as much as author implies.
  * Brad Elfron: He should have talked about bias more. How much bias do proposed methods introduce?
* On "prediction VS understanding" distinction: If we take prediction out of stats, what is left? What makes "understanding"?
  * Controlled experiments, causal inference. At least for this controlled setting you might be sure that the IV has some influence (or not).
