# Protocol 2019-04-09, Tuesday

## General topics

* First meeting. Organizational questions.
* Starting point of our meetup: How science should be made? Why haven't methods in social sciences worked that well so far?

## Papers/Technicalities

* Meehl, P. E. (1978). Theoretical risks and tabular asterisks: Sir Karl, Sir Ronald, and the slow progress of soft psychology. Journal of consulting and clinical Psychology, 46(4), 806.

## Protocol

* List of shortcuts used in the protocol
  * SoSci = social science(s)

### SESSION START

* Organizational questions
* Goals, format, place, etc.
  * ...are to be discussed further in private.
* Evtl. registering our meetup as Projektutorium in Wintersemester 2018/19 [Deadline application for HU Berlin: 2 June 2019]

### Discussion Paper Meehl

* Opening statement: Why (soft) psychology and social sciences in general failed to deliver theories/models of high or even any scientific/practical merit? How can we fix it?
	* Meehl's list of 20 points: Apart from all other problems, there is a number of intrinsic qualities of the object of study (namely, human behavior), that make it almost impossible for SoSci to develop such theories.
	* Plenum: The tragedy of this statement is that even if we organize our scientific endeavor in the best way (open science, pre-registration, etc.) and have unlimited resources (huge sample sizes, processing power, etc.), we would nevertheless not be able to overcome these intrinsic barriers.
	* Also see Meehl's conclusion/prediction in the end of the paper: Psychology will probably never solve this problem and we won't see any useful theories.
	* Some very good and tested theories in subfields such as perception psychology but findings rarely relevant for the social policies, for example.
* Historical remark
	* Question: Regarding the need for strict formal models - is this a new problem in psychology or has it always been a goal?
	* Plenum: The founders of (experimental) psychology, (e.g., Wundt, Helmholtz) explicitly wrote that we need to apply rigorous scientific/math methods of hard sciences to human behavior - thus, the new field of psychology. They wanted to go for that "risky" predictions and strong test of the theories.
	* In some aspects, we are even degrading from that viewpoint. Perhaps the development of computers, and thus statistical software, prevented us from complete failure, but it may end soon. 
	* Also, there is a trend now (cf. open science foundation, replication) that disproving/refuting a theory is even more.
	
### Possible solutions

* More variables in the model?
	* Roughly speaking, researchers could consider more variables in their models to check for all influences.
	* Contra-point: Yes, but what about all the problems that arise with that approach?
		* (Cf. Meehl's point about "sheer number of variables")
		* High-dimensional data --> multicollinearity, number of interaction terms, etc. You won't be able to look through all this.
		* Moreover, there is an infinite number of possible variables that you can theoretically construct and measure. How would you decide how many and which one to choose?
		* Argument: If we have infinite number of variables, and thus models/theories, you can not say that your theory is more "versimile," than others, because you haven't tested them all.
* More attention to auxialiaries?
	* See Meehl's argument that we always testing the conjunction and not the core theory only (cf. Dunheim-Ding paradox).
	* Point: Psychologist researchers do not pay enough attention to the auxiliary theories. If you, say, use some construct in your experiment, you must be able to have a good theory about it as well (why I used this measure? What are its qualities?) 
* Using an obligatory set of standards, at least within a (sub-)field?
  * Point: If there is a number of standards in a field that every researcher must follow, it may increase the quality of research.
    * Example: Every analysis in sociology must include control for genetics.
    * Side remark by a graduate of sociology: Almost no mention of genetics throughout the whole Bachelor in sociology (HU Berlin). Although, some large longitudinal studies (such as Socio-Economic Panel in Germany) now consider sequencing participants' genomes.
  * Counter-point: Will it not lead to dogmatism in science or persistence of outdated paradigms (cf. Thomas Kuhn)? Who will decide about the standards? What if these standards are inefficient/outdated or plain wrong?
* Testing a theory/construct across several methods, measurement instruments, statistical models, samples, etc.?
  * Point: Validation a construct/relationship across several methods/samples/etc. is a very strong test of and an argument for its validity.
  * See related terms: Model independence, cross-validation, triangulation
  * Very few examples of such constructs in (soft) psychology.
    * g factor of intelligence: A factor "explaining" ~50% of variance comes up almost in every test with abstract problems.
    * Semantic/lexical hypothesis of personality.
    
### Further points

* Distinction between two approaches: Testing a theory VS fitting a model to the data.
  * In psychology, we rarely test the theory, rather "just" collect data and fit a model.
  * How we could come to conclusion about the definitions in SoSci?
    * Compare Meehl's example of X ways of estimating the Avogadro number with terms from behavior science like "aggression"
    * Avogadro number is "always out there," it is a fundamental law in our nature (cf. Comte's hierarchy of science). Thus, it has been just "waiting to be discovered".

### Debating Time

#### Aaron thinks parsimonie over time works like that:

* Researcher A has theory A which has a parsimonie of 10 (on a abitrary unnkown scale)
* Researcher B has theory B which has the same parsimonie and sets out to explain the same phenomenon
* Researcher A goes a head and gathers data via an experiment on predictions from theory A
* Researcher B looks at the results and changes his Theory; based on a effect he had expected, but was not supported by data, he removes the effect from theory B creating B'
* Researchers theory B' has now a apparently parsimonie of 9

Aaron concludes that theory B' is not more corroberated then theory A. Becouse here theorie B' is theory B' in disguise. It follows that the parsimonie of theory B, therefore 10, applies and only for the future, parsimonie of B' (9) applies. However in this example A & B set out equally parsimonious. It is a truism that implicit, unformalised theories are less parsimonious then there formal conterpart. It can be argued that researchers theory B, before looking at the data is of the former kind and therefore less parsimonious than theory A. These commonly found circumstances are what justyfies the parsimonie 'bonus' for theory A.


#### Max's Answer

"* Researcher B looks at the results and changes his Theory; based on a effect he had expected, but was not supported by data, he removes the effect from theory B creating B'
* Researchers theory B' has now a apparently parsimonie of 9" - *One problem here is that you again set theory and hypothesis to be equal, which only works for the weekest of all theories, that have as many sentences as hypotheses. However, to illustrate your example: if the 10 parameters are 10 proposed causal effects, and theory A says all 10 exist, Theory B' (proposed after the fact) says 9 of them exist, I would agree that theory B' is more parsimonious. You may want to object "But aren't we just incorporating noise in our theory then?" - maybe we are, maybe not, there is a chance this is noise we sample. But the important thing is: **the sampling process does not care about the point of time we stated our theory!** Why should it be more probable that the 0 effect proposed after the fact is noise than that one of the other 9 non-zero effects proposed beforehand are noise?*

I think the example you give Aaron is, again, one of a very week theory - 10 parameters, 10 hypotheses and 10 sentences in the theory - to be honest, this is not really a theory at all, because it entails all "BeobachtungssÃ¤tze". No abstraction at all! Of course, often an easy example is a good starting point, but in this case I would argue it is almost pointless to argue about the parsimony of those "theories". Try to give a more complex example, and you will see, that your approach does not work anymore. Why? Because:

If we have **theory -> hypothesis -> data**, and the theory is a real theory - which mean it **explains** or **predicts** relationships between variables **in a multitude of situations**, we would like to know if the parameter of interest is estimated from **noise** or **systematic** data components. If it is noise, we will not expect the theory to work in the future. If it is systematic, it is stronger corroborated. If we now take into account, as mentioned earlier, that **the sampling process does not care about the point of time we stated our theory!**, we have to come to the conclusion that the probability of basing our theory on noise or on systematic data does not depend on the time we stated our theory, therefore the expected accuracy of future predictions does not depend on it and therefore the amount of corroboration can not depend on it. But **if parsimony would depend on the point of time we stated our theory, corroboration would depend on it**. Which directly contradicts my previous argument.


### Reading suggestions
* Meehl, P. E. (1990). Appraising and amending theories: The strategy of Lakatosian defense and two principles that warrant it. Psychological inquiry, 1(2), 108-141.
	* Mentioned by one member as the paper on philosophy of science, if you had a chance to read only one.

